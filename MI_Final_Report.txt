═══════════════════════════════════════════════════════
         多重插补 (MI) 分析最终报告
         专用于 IPW 权重构建
═══════════════════════════════════════════════════════

生成时间: 2025-12-09 00:24:10 
1. 数据清洗
─────────────────────────────────────
原始样本数        : 1124 
清洗后样本数      : 1124 
有随访数据        : 530 ( 47.2 %)
处理含符号变量    : 8 个
Winsorize 变量数  : 27 个

2. 变量选择策略（基于单变量筛选优化）
─────────────────────────────────────
插补变量总数      : 90 
  ├─ 结局变量     : 2 个 (LVESV_fu, EF_fu)
  ├─ 暴露变量     : 19 个 (金属)
  ├─ IPW核心协变量: 13 个 (p<0.05, 缺失<5%)
  ├─ IPW辅助协变量: 18 个 (临床重要, 缺失<20%)
  └─ 其他辅助变量 : 25 个

明确排除的变量:
  ├─ 高共线性      : CK (VIF=9.76), CKMB (VIF=8.51), LVESV_baseline (VIF=79.87)
  ├─ 高缺失梗死部位: Inferior_MI, Anterior_MI 等 (~47%缺失)
  ├─ 高缺失冠脉变量: LAD, LCX, RCA 等 (~47%缺失)
  └─ 结局变量      : LVEDV_fu (不应插补)

3. 分层缺失率阈值
─────────────────────────────────────
  ├─ IPW核心协变量: ≤ 5 % (极严格)
  ├─ IPW辅助协变量: ≤ 20 % (严格)
  ├─ 结局变量     : ≤ 40 % (中等)
  └─ 其他辅助变量 : ≤ 50 % (宽松)

4. 变量质量诊断
─────────────────────────────────────
质量问题移除变量  : 1 个
  ├─ 高相关变量   : 1 个 (|r|>0.95)

5. 多重插补参数
─────────────────────────────────────
插补数据集数 (m)  : 20 
最大迭代数 (maxit): 10 
插补方法分布      :
  ├─ PMM          : 59 个 (连续变量 + 低事件数二元变量)
  ├─ logreg       : 6 个 (二元变量)
  ├─ polr         : 2 个 (有序变量)
  ├─ 被动插补     : 1 个 (BMI)
  └─ 不插补       : 24 个 (ID, has_fu_echo, 结局变量)

6. 插补质量评估
─────────────────────────────────────
插补完成情况      :
  ├─ 完全插补变量 : 66 个
  ├─ 部分插补变量 : 0 个
  └─ 未插补变量   : 1 个 (目标/结局变量)

IPW核心变量插补质量:
  -  STEMI :  1124  缺失 →  0 % 插补
  -  CKMB :  11  缺失 →  100 % 插补
  -  NTproBNP_peak :  3  缺失 →  100 % 插补
  -  resident :  2  缺失 →  100 % 插补

插补日志事件      : 1 个
  ✓ 日志事件数可接受

高缺失但完全插补变量 (>30%):
  -  BMI  ( 100 % 缺失)
  -  LVESV_fu  ( 53 % 缺失)
  -  EF_fu  ( 48.8 % 缺失)
  ⚠ 提示: 这些变量的插补质量需要特别验证

7.  输出文件清单
─────────────────────────────────────
核心输出:
  ├─ mice_imputation_final.rds      : 插补对象 (用于IPW建模)
  └─ data_for_mi_final.rds          : 插补前数据

诊断文件:
  ├─ imputation_diagnostic_report.csv : 插补完成率
  ├─ missing_summary_for_mi.csv       : 缺失率汇总
  ├─ excluded_variables_summary.csv   : 排除变量清单
  ├─ removed_high_missing_vars.csv    : 高缺失移除清单
  ├─ high_correlation_pairs.csv       : 高相关变量对
  ├─ mice_logged_events.csv           : 插补日志事件
  └─ symbol_cleaning_log.csv          : 符号清洗日志

图形文件:
  ├─ missing_pattern_before_mi.png    : 插补前缺失模式
  ├─ mi_convergence. png               : 收敛性诊断图
  ├─ mi_density.png                   : 插补值密度图
  └─ mi_stripplot_key_vars.png        : IPW核心变量条带图

8. 关键决策记录
─────────────────────────────────────
变量选择决策:
  ✓ 保留 cTnIpeak (VIF=2.38), 移除 CK/CKMB (VIF>5)
  ✓ 保留 EF_baseline + LVEDV_baseline, 移除 LVESV_baseline (VIF=79.87)
  ✓ 移除梗死部位变量 (Inferior_MI等, 47%缺失)
  ✓ 移除冠脉变量 (LAD/LCX/RCA, 47%缺失)
  ✓ LVEDV_fu 不插补 (结局变量)

插补方法决策:
  ✓ 连续变量: PMM (稳健且保持分布)
  ✓ 二元变量: logreg (事件数≥10) 或 PMM (事件数<10)
  ✓ 有序变量: polr (水平样本数≥5) 或 PMM (否则)
  ✓ BMI: 被动插补 (基于height和weight)

预测矩阵决策:
  ✓ 使用 quickpred() 稀疏化 (mincor=0.2, minpuc=0.3)
  ✓ 强制包含IPW核心变量作为预测因子
  ✓ 最终密度: 10.1 %

9. 下一步分析建议
─────────────────────────────────────
IPW权重构建:
  1. 加载插补对象: readRDS('outputs/mice_imputation_final.rds')
  2. 对每个插补数据集拟合倾向性评分模型:
     formula: has_fu_echo ~ pPCI + STEMI + ST_dev + cTnIpeak +
              NTproBNP_peak + EF_baseline + LVEDV_baseline +
              GRACE_in_str + age + hypertension + resident + .. .
  3. 计算稳定化IPW权重
  4.  检查协变量平衡 (bal. tab, SMD<0.1)
  5. 截断极端权重 (99th percentile)

结局模型:
  1.  计算 ΔLVEDV = LVEDV_fu - LVEDV_baseline
  2. 拟合加权线性/logistic回归: ΔLVEDV ~ metals + covariates
  3. 使用Rubin规则合并多重插补结果
  4. 报告合并后的估计值与标准误

质量控制:
  ✓ 检查IPW权重分布 (范围、极端值)
  ✓ 敏感性分析: 比较不同m值的结果稳定性
  ✓ 完整案例分析: 对比IPW加权与未加权结果
  ✓ 检查高缺失变量插补质量 (LVESV_fu, EF_fu)

10. 重要注意事项
─────────────────────────────────────
插补局限性:
  ⚠ LVESV_fu 和 EF_fu 缺失率>40%, 插补不确定性大
  ⚠ 金属暴露全部缺失同一批样本 (167例), 可能MNAR
  ⚠ BMI完全缺失但通过被动插补重构, 需验证合理性

统计推断:
  ⚠ IPW权重放大方差, 置信区间会变宽
  ⚠ MI与IPW的双重不确定性需正确传递
  ⚠ 使用稳健标准误 (survey包的svyglm)

═══════════════════════════════════════════════════════
                    报告结束
═══════════════════════════════════════════════════════
